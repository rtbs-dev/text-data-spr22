---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region hideCode=false hideOutput=false hidePrompt=false -->
# Alexander Adams

# Homework II: Goals and Approaches

# PPOL628 Text as Data: Computational Linguistics
<!-- #endregion -->

```python hideCode=false hideOutput=false hidePrompt=false
#Run dvc
!dvc pull
```

```python hideCode=false hideOutput=false hidePrompt=false
from bertopic import BERTopic
import pandas as pd
```

```python
#Import data
mtg = (pd.read_feather('../../../data/mtg.feather'))
mtg = mtg.dropna(subset = ['flavor_text']).reset_index(drop=True)
# .head()[['name','text', 'mana_cost', 'flavor_text','release_date', 'edhrec_rank']] 
mtg.shape
```

<!-- #region hideCode=false hideOutput=false hidePrompt=false -->
## Part I: Unsupervised Exploration
<!-- #endregion -->

```python hideCode=false hideOutput=false hidePrompt=false
#Load in topic model generated by topic_model.py
topic_model = BERTopic.load('hw2_bert_model')
```

```python hideCode=false hideOutput=false hidePrompt=false
topics_list = topic_model.get_topics()
len(topic_model.get_topics())
```

<!-- #region hideCode=false hideOutput=false hidePrompt=false -->
When the model is fitted on the flavor text and no additional parameters are specified, BERT finds 977 topics in this corpus. Those topics are plotted in two dimensions below:
<!-- #endregion -->

```python hideCode=false hideOutput=false hidePrompt=false
topic_model.visualize_topics(topics_list)
```

```python hideCode=false hideOutput=false hidePrompt=false
#This is necessary because we're working with an imported model
#See: https://github.com/MaartenGr/BERTopic/issues/498#issuecomment-1097932864
probs = topic_model.hdbscan_model.probabilities_
topics = topic_model._map_predictions(topic_model.hdbscan_model.labels_)
```

<!-- #region hideCode=false hideOutput=false hidePrompt=false -->
If I reduce the number of topics by about 99% (from 977 down to 10), what does the output look like?
<!-- #endregion -->

```python hideCode=false hideOutput=false hidePrompt=false
#Reduce the number of topics
new_topics, new_probs = topic_model.reduce_topics(mtg['flavor_text'], topics, probs, nr_topics = 10)
```

```python hideCode=false hideOutput=false hidePrompt=false
#Plot the ten topics
topic_model.visualize_topics()
```

<!-- #region hideCode=false hideOutput=false hidePrompt=false -->
I can now display the top words associated with each of the topics. The documentation for BERTopic says that topic #-1 refers to outliers and should be ignored.
<!-- #endregion -->

```python hideCode=false hideOutput=false hidePrompt=false
topic_model.get_topic_info()
```

<!-- #region hideCode=false hideOutput=false hidePrompt=false -->
Based on these words, it looks like the topics are more-or-less as follows:

    1: Dragons
    2: Goblins
    3: Unknown
    4: Light and darkness
    5: Unknown
    6: Unknown
    7: Unknown
    8: Unknown
    9: Nature and civilization
    10: Maybe elemental transmutation? (That sounds like something that would be present in the game.)
    
Topics 3, 5, 6, 7, and 8 are clearly associated with particular proprietary characters, events, locations, or items in the game; while these topics may be internally coherent within the Magic: The Gathering domain (and may make sense to those more familiar with this subject matter), they are largely inscrutable to non-experts, insofar as these top few words are considered. 

Next, I create a dynamic topic model:
<!-- #endregion -->

```python hideCode=false hideOutput=false hidePrompt=false
dynamic_topics = topic_model.topics_over_time(mtg['flavor_text'],
                                              new_topics, 
                                              mtg['release_date'])
```

```python hideCode=false hideOutput=false hidePrompt=false
topic_model.visualize_topics_over_time(dynamic_topics,
                                       topics=[0,1,2,3,4,5,6,7,8,9],
                                       width = 950)
```

If the above plot is filtered to just include the topics with recognizable words, certain trends become somewhat visible. For one, the "Dragon" and "Goblin" topics have significant overlap, starting around 2003 when they experience local spikes back to back. The "Dragon" topic is consistently more prominent and has greater local maxima, which could indicate that dragons are more common in the set of available cards, or that they are associated with a large number of other entities. 

The "light and darkness" topic experienced a peak around 2017, and again in late 2019. Before these, this topic had only been associated with notable peaks in 2008 and 1996-7. 

When all ten topics are included in the plot, we can see that while topics 2 and 6 have local peaks greater than the greatest "Dragon" peak, both of those peaks occurred prior to 2000, and there have been no similar occurrences since then. It is possible that this could indicate a shift away from more propietary concepts in cards and toward more universally-known fantasy tropes and items. Those topics could also be associated with particular collections or decks of cards which have been discontinued or which are part of now-ended series.


---


# Part II: Supervised Classification

### a) Multiclass Classification


First, the multiclass model is loaded, the data is preprocessed to match the format used in the pipeline, and a confusion matrix is generated.

```python
import joblib
import numpy as np
from sklearn.metrics import (confusion_matrix, multilabel_confusion_matrix, 
precision_recall_fscore_support, classification_report)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
```

```python
#Load the trained multiclass pipeline
multiclass_model = joblib.load('multiclass_pipe.pkl')
```

```python
#Drop rows with missing values in the variables of interest
mtg = mtg.dropna(subset = ['flavor_text', 'text', 'color_identity']).reset_index(drop=True)
```

```python
#Create numeric labels based on values of colors
#New values: X = multiclass, Z = NaN
mtg['color_label'] = [np.array(['X']) if len(x) > 1 else x for x in mtg['color_identity']]
mtg['color_label'] = [np.array(['Z']) if len(x) == 0 else x for x in mtg['color_label']]
mtg['color_label'] = np.concatenate(mtg['color_label'])
```

```python
#Merge labels into MTG data frame
labels = pd.DataFrame(mtg['color_label'].unique()).reset_index()
#Add one because zero indexed
labels['index'] = labels['index']+1
labels.columns = ['label', 'color_label']
mtg = labels.merge(pd.DataFrame(mtg), how = 'right', on = 'color_label')
```

```python
#Select labels as targets
y = mtg['label']

#Select text columns as features
X = mtg[['text', 'flavor_text']]

#Training test split 75/25
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)
```

```python
predicted_mc = multiclass_model.predict(X_test)
```

```python
multiclass_cm = pd.DataFrame(confusion_matrix(y_test,predicted_mc.round()))
multiclass_cm.index = np.unique(mtg['color_label'])
multiclass_cm.columns = np.unique(mtg['color_label'])
multiclass_cm
```

I was somewhat unsure how to tackle this, so I treated "multiclass" as a class itself and labeled it "X". I also labeled those cards which were missing a color with "Z". I know that this was not how this problem was intended to be solved, and that as such I likely achieved different outcomes from what was expected or desired. 

Based on the raw confusion matrix, it appears that the Linear Support Vector Classifier was reasonably successful at predicting colors for cards, as well as predicting when a card had no color and when it had multiple colors. However, in order to properly assess the classifier results, it is necessary to calculate precision, recall, and the F-score.

```python
mc_evals = pd.DataFrame()
for x in np.unique(y_test):
    mc_evals[x] = precision_recall_fscore_support(y_test,
                                                 predicted_mc,
                                                 labels = [x],
                                                 average = 'macro')
```

```python
mc_evals.columns = np.unique(mtg['color_label'])
mc_evals.index = ['precision','recall','f-score','support']
mc_evals
```

The output above indicates that for all labels in this problem, precision, recall, and f-score were approximately 0.95. There is a high degree of consistency across classes, which suggests that the text and flavor text provided with each card are useful predictors for each class and for all classes. A precision score of 0.95 means that 95% of the instances classified with a particular label actually correspond to that label. A recall score of 0.95 means that 95% of the instances in the dataset with a particular label were classified with that label. The F-score is a measure of accuracy which depends on both precision and recall. A high F-score means that the classifier or model is highly accurate at assigning correct labels and minimizing both false positives and false negatives.


Next, the multilabel model is loaded, and that confusion matrix is generated:

```python
multilabel_model = joblib.load('multilabel_pipe.pkl')
```

```python
for letter in np.unique(np.concatenate(np.array(mtg['color_identity']))):
    mtg["is_"+letter] = [1 if letter in x else 0 for x in mtg['color_identity']]
```

```python
letters = mtg.columns.str.contains('is_')

mtg['labels'] = mtg[mtg.columns[letters]].values.tolist()
```

```python
#Select labels as targets
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(mtg['labels'])
#Select text columns as features
X = mtg[['text', 'flavor_text']]

#Training test split 75/25
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)
```

```python
predicted_ml = multilabel_model.predict(X_test)
```

```python
multilabel_cm = multilabel_confusion_matrix(y_test,predicted_ml.round())
multilabel_cm
```

```python
precision_recall_fscore_support(y_test, predicted_ml, average = 'macro')
```

Based on the above results, it appears that the multilabel classifier was *extremely* accurate, with precision scores, recall scores, and F-scores all above 0.99. While I am not 100% sure how to interpret the multilabel confusion matrix, here is my current understanding:

The first matrix above shows the output with respect to 0 (i.e. a given color is not in the color identity). There were no false positives, and only 6 false negatives. This matrix is heavily weighted in favor of true negatives, meaning predictions of "not-0" for an element of a multiple label which was "not-0". 

The second matrix shows the output with respect to 1 (i.e. a given color is in the color identity). Again, the number of correct predictions (true positives and true negatives) is overwhelming. The most common type of error in this matrix is a false negative, which in this matrix means that there were about 100-130 instances where the correct label was 1 and the prediction was "not-1".

For this part, I only used unigrams, and the only method of preprocessing employed in the pipeline was the TfidfVectorizer. This vectorizer first converts each corpus (in this case, each of the "text" and "flavor_text" columns) into a document-term matrix, then removes stopwords according to scikit-learn's inbuilt set of stopwords. It then calculates the TF-IDF (term frequency-inverse document frequency) value for each remaining word. These scores are then used as features to predict and classify instances in this data set (in this case, to assign color labels to different Magic: The Gathering cards). The two confusion matrices and the associated performance metrics I achieved indicate that even this minimal amount of preprocessing was sufficient to train highly accurate classifier. The accuracy may also be due in part to the fraction of the data used for training and testing. I used a 75/25 training-test split for both classifiers in this section; it is possible that a different ratio could have produced lower levels of precision or recall.


---


# Part III: Regression


To accomplish this task, I used the GridSearchCV function available through scikit-learn. I selected four regressor models (LASSO regression, Decision Tree, K-Nearest Neighbors, and Random Forest), specified certain values to test for different parameters, and then fit the models to the data to find the most accurate regressor. My target variable is `edhrec_rank`, which I understand encodes the popularity of a Magic: The Gathering card at the time the dataset was compiled. My feature variables were as follows: "text", "flavor_text", dummies encoding the presence of a specific color label in the "color identity" column, and dummies encoding either "uncommon" or "rare" values of the "rarity" column. I preprocessed the text columns with the TfidfVectorizer used in part II; the other feature columns exclusively take on values of 0 or 1, and so do not require preprocessing.

```python
estimator = joblib.load('regression_GridSearch.pkl')
```

```python
mtg = mtg.dropna(subset = ['flavor_text', 'text', 'color_identity', 'edhrec_rank', 'rarity']).reset_index(drop=True)
mtg = pd.get_dummies(mtg, columns = ['rarity'])
```

```python
#Specify X and Y
y = mtg['edhrec_rank']

X = mtg[['text','flavor_text','is_B','is_G', 'is_R', 'is_U', 'is_W', 'rarity_uncommon', 'rarity_rare']]
```

```python
estimator.fit(X,y)
```

```python
regressor_predictions = estimator.predict(X)
```

```python
plotting_data = pd.DataFrame({'actual': y,
                              'predicted': regressor_predictions})
```

```python
from plotnine import *
```

```python
(
    ggplot(plotting_data, aes(x='actual', y='predicted'))
    + geom_point(alpha = 0.1,
                 color = 'green')
    + geom_abline(intercept=0, # set the y-intercept value
                  slope=1,     # set the slope value
                  color = 'black',
                  size = 1
                 )
    + labs(x='Ground Truth',
           y='Predicted Rank',
           title='Predicting Rank of MTG Cards \nusing 5-Nearest Neighbors')
)
```

The above graph plots the predicted ranks against the actual values provided in the dataset. The black diagonal line indicates perfect performance, where true value = prediction. Clearly, there is substantial variation, though it seems to follow a normal distribution both above and below the black line. In the process of testing the pipeline for this part, I found that K-Nearest Neighbors where k = 5, the strongest performing regressor in the Grid Search, produced a negative mean squared error of -2280000, at least. Even the most accurate methods I tested were largely inaccurate. The two possible conclusions I could draw from this are: 
    -1: The features I selected have limited predictive power and do not contribute strongly to or majorly affect the rank of a given card, and
    -2: The specific regressor models I chose are not well suited to this data set.
    
In either case, the solution is effectively further experimentation, though such experimentation is not without possible challenges. Regarding the feature variables, I selected these ones because they had relatively low levels of missingness. Other variables like "power" may be more predictive, but "power" was also missing for over 14,000 of 29,000 observations.

As for the models themselves, I am surprised that the K-Nearest Neighbors regressor performed the best, and that it achieved optimal performance with only 5 neighbors. I expected the Random Forest regressor to perform better; that has been the case with other data tasks and projects I have worked on. If a LASSO regressor did not perform well, then a ridge regressor likely will not perform well either. Other methods like boosting and bootstrapping could increase the viability of some of the tree based methods, but this is uncertain. 


---


# Part 4: Experimentation


|Path        |  Metric    | HEAD     |workspace |   Change     |
| ---------- | -----------|----------|----------|--------------|
|metrics.json|  f1-score  | 0.81529  |0.81189   |   -0.0033999 |
|metrics.json|  precision | 0.81585  |0.81229   |   -0.0035581 |
|metrics.json|  recall    | 0.81611  |0.81296   |   -0.0031493 |

________________________

|Path        | Param                              | HEAD  |  workspace  |  Change |
| ---------- | -----------------------------------|-------|-------------|--------------|
|params.yaml | preprocessing.max_min_docs.largest | 0.8   |  0.9        |  0.09999999999999998 |


|Path          |Metric     |HEAD     |workspace    |Change|
| ---------- | -----------|----------|----------|--------------|
|metrics.json  |f1-score   |0.81529  |0.81545      |0.00016295|
|metrics.json  |precision  |0.81585  |0.81599      |0.00013555|
|metrics.json  |recall     |0.81611  |0.81633      |0.00022495|

____________________________________

|Path         |Param                               |HEAD    |workspace    |Change|
| ---------- | -----------|----------|----------|--------------|
|params.yaml  |preprocessing.max_min_docs.largest  |0.8     |0.7          |-0.10000000000000009|


Path          Metric     HEAD     workspace    Change
metrics.json  f1-score   0.81529  0.62962      -0.18568
metrics.json  precision  0.81585  0.63716      -0.17869
metrics.json  recall     0.81611  0.63997      -0.17613

Path         Param                                HEAD    workspace    Change
params.yaml  preprocessing.max_min_docs.smallest  10      100          90


# Appendix A: BERTopic Code

```python
#Filename: topic_model.py
from bertopic import BERTopic
import pandas as pd

#Read in data, drop NAs, reset index
mtg = (pd.read_feather('../../../data/mtg.feather')).dropna(subset = ['flavor_text']).reset_index(drop=True)
#Instantiate BERT model
topic_model = BERTopic()
#Fit model to flavor text
topics, probs = topic_model.fit_transform(mtg['flavor_text'])
#Save model
topic_model.save('hw2_bert_model')
```

# Appendix B: Multiclass Code

```python
import joblib
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC

#Read in data
mtg = (pd.read_feather('../../../data/mtg.feather'))
#Drop rows with missing values in the variables of interest
mtg = mtg.dropna(subset = ['flavor_text', 'text', 'color_identity']).reset_index(drop=True)

#Create numeric labels based on values of colors
#New values: X = multiclass, Z = NaN
mtg['color_label'] = [np.array(['X']) if len(x) > 1 else x for x in mtg['color_identity']]
mtg['color_label'] = [np.array(['Z']) if len(x) == 0 else x for x in mtg['color_label']]
mtg['color_label'] = np.concatenate(mtg['color_label'])

#Merge labels into MTG data frame
labels = pd.DataFrame(mtg['color_label'].unique()).reset_index()
#Add one because zero indexed
labels['index'] = labels['index']+1
labels.columns = ['label', 'color_label']
mtg = labels.merge(pd.DataFrame(mtg), how = 'right', on = 'color_label')

#Select labels as targets
y = mtg['label']

#Select text columns as features
X = mtg[['text', 'flavor_text']]

#Training test split 75/25
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)

#Preprocess text (this took several hours to debug and I am honestly not joking)
preprocess = ColumnTransformer(transformers=[('text', TfidfVectorizer(), 'text'),
                                             ('flavor_text', TfidfVectorizer(), 'flavor_text')])

#Create pipeline with preprocessing and linear SVC
pipe = Pipeline([
    ('preprocess', preprocess),
    ('LinearSVC', LinearSVC())
])

#Fit pipe to training data
fitted_pipe = pipe.fit(X_train, y_train)

#Export pickeled pipe
joblib.dump(fitted_pipe, 'multiclass_pipe.pkl')
```

# Appendix C: Multilabel Code

```python
import joblib
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier

#Read in data
mtg = (pd.read_feather('../../../data/mtg.feather'))
#Drop rows with missing values in the variables of interest
mtg = mtg.dropna(subset = ['flavor_text', 'text', 'color_identity']).reset_index(drop=True)

for letter in np.unique(np.concatenate(np.array(mtg['color_identity']))):
    mtg["is_"+letter] = [1 if letter in x else 0 for x in mtg['color_identity']]

letters = mtg.columns.str.contains('is_')

mtg['labels'] = mtg[mtg.columns[letters]].values.tolist()

#Select labels as targets
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(mtg['labels'])
#Select text columns as features
X = mtg[['text', 'flavor_text']]

#Training test split 75/25
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)

#Preprocess text (this took several hours to debug and I am honestly not joking)
preprocess = ColumnTransformer(transformers=[('text', TfidfVectorizer(), 'text'),
                                             ('flavor_text', TfidfVectorizer(), 'flavor_text')])

#Create pipeline with preprocessing and linear SVC
pipe = Pipeline([
    ('preprocess', preprocess),
    ('classifier', OneVsRestClassifier(SVC()))
])

#Fit pipe to training data
fitted_pipe = pipe.fit(X_train, y_train)

#Export pickeled pipe
joblib.dump(fitted_pipe, 'multilabel_pipe.pkl')
```

# Appendix D: Rank Regressor

```python
import joblib
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import Lasso

#Read in data
mtg = (pd.read_feather('../../../data/mtg.feather'))
#Drop rows with missing values in the variables of interest
mtg = mtg.dropna(subset = ['flavor_text', 'text', 'color_identity', 'edhrec_rank', 'rarity']).reset_index(drop=True)

#Create dummy columns based on color labels
for letter in np.unique(np.concatenate(np.array(mtg['color_identity']))):
    mtg["is_"+letter] = [1 if letter in x else 0 for x in mtg['color_identity']]
    
#Feature variables: text, flavor text, color labels, rarity
mtg = pd.get_dummies(mtg, columns = ['rarity'])

#Specify X and Y
y = mtg['edhrec_rank']

X = mtg[['text','flavor_text','is_B','is_G', 'is_R', 'is_U', 'is_W', 'rarity_uncommon', 'rarity_rare']]

#70/30 training-test split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3)

#(1) Choose a number of folds and specify a random state
fold_generator = KFold(n_splits=5, shuffle=True)

#(2) Specify a preprocessing step for the text columns
preprocess = ColumnTransformer([('text', TfidfVectorizer(), 'text'),
                                ('flavor_text', TfidfVectorizer(), 'flavor_text')])

#(3) Create the model pipe
pipe = Pipeline(steps=[('pre_process', preprocess),
                       ('model',None)])

#(4) Instantiate the search space
search_space = [
    # Naive Bayes Classifier
    {'model' : [Lasso()]},
    
    # K-Nearest-Neighbors, also specifying values of K to test
    {'model' : [KNeighborsRegressor()],
    'model__n_neighbors':[5,10,25,50]},
    
    # Decision Tree, also specifying depth levels to test
    {'model': [DecisionTreeRegressor()],
    'model__max_depth':[2,3,4]},
    
    # Random forest, also specifying depth levels, numbers of estimators, and numbers of features to test
    {'model' : [RandomForestRegressor()],
    'model__max_depth':[2,3,4],
    'model__n_estimators':[500,1000,1500],
    'model__max_features':[3,4,5]},
]

#Assemble the GridSearch
search = GridSearchCV(pipe, 
                      search_space, 
                      cv = fold_generator,
                      scoring='neg_mean_squared_error',
                      n_jobs=-1)

#Fit the GridSearch
search.fit(X_train, y_train)

#Save the best estimator 
joblib.dump(search.best_estimator_, 'regression_GridSearch.pkl')
```
