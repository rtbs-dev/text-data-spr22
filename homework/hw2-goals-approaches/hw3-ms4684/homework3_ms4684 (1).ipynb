{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d4833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bertopic import BERTopic\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN_reg\n",
    "from sklearn.tree import DecisionTreeRegressor as DT_reg\n",
    "from sklearn.ensemble import RandomForestRegressor as RF_reg\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "(pd.read_feather('C:/Georgetown University/Courses/Spring Semester 2022/Text As Data/text-data-spr22/data/mtg.feather')# <-- will need to change for your notebook location\n",
    " .head(2)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc36ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store full data\n",
    "df = (pd.read_feather('C:/Georgetown University/Courses/Spring Semester 2022/Text As Data/text-data-spr22/data/mtg.feather')  \n",
    ")\n",
    "\n",
    "# check shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92af5dc",
   "metadata": {},
   "source": [
    "### Part 1: Unsupervised Exploration\n",
    "\n",
    "Investigate the BERTopic documentation (linked), and train a model using their library to create a topic model of the flavor_text data in the dataset above.\n",
    "\n",
    "- In a topic_model.py, load the data and train a bertopic model. You will save the model in that script as a new trained model object\n",
    "- add a \"topic-model\" stage to your dvc.yaml that has mtg.feather and topic_model.py as dependencies, and your trained model as an output\n",
    "- load the trained bertopic model into your notebook and display\n",
    "    - the topic_visualization interactive plot see docs\n",
    "    - Use the plot to come up with working \"names\" for each major topic, adjusting the number of topics as necessary to make things more useful.\n",
    "    - Once you have names, create a Dynamic Topic Model by following their documentation. Use the release_date column as timestamps.\n",
    "    - Describe what you see, and any possible issues with the topic models BERTopic has created. This is the hardest part... interpreting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca43b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained BERTopic model\n",
    "topic_model = BERTopic.load(\"flav_text_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access frequent topics\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e2dfc",
   "metadata": {},
   "source": [
    "-1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8880864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a017fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store topic frequency\n",
    "freq_topics = topic_model.get_topic_info().iloc[1: , :] # remove row with outliers (where Topic = -1)\n",
    "\n",
    "# view percentiles of Count/frequency\n",
    "freq_topics.Count.quantile([0.25,0.5,0.75,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a122d",
   "metadata": {},
   "source": [
    "Will select topics whose Count is in the 99th percentile. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386c22e",
   "metadata": {},
   "source": [
    "#### Interactive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b92e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all topics \n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b81c6",
   "metadata": {},
   "source": [
    "It's very hard to interpret 800+ topics, so I am going to select and visualize topics that have a frequency in the top percentile. Assumption: high frequency topics are representative of the main 'topic clusters'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e2ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view topics with freq in the top percentile\n",
    "freq_topics.loc[freq_topics.Count > freq_topics.Count.quantile(0.99)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fcd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view intertopic distance map\n",
    "topic_model.visualize_topics(topics = [-1,0,1,2,3,4,5,6,7,8,9,10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba994fd",
   "metadata": {},
   "source": [
    "In order to name these topics, I will visualize them as bar charts that include the top 9 words in each topic. (I tried including the top 10 words but doing that only displays alternate written words which makes it difficult to interpret)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6470588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(topics = [0,1,2,3,4,5,6,7,8,9,10], n_words = 9) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd85e8b2",
   "metadata": {},
   "source": [
    "I'm not too familiar with these cards, but through Google searches of the top few words, I was able to come up with what I think are good topic names. I have added supporting links as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68860e62",
   "metadata": {},
   "source": [
    "- Topic 0 - Based on the top words (which show up in 'Phyrexia creature' cards in Google searches), this topic seems to capture the set 'New Phyrexia'.\n",
    "- Topic 1 - Sword of Sinew and Steel (https://www.cardkingdom.com/mtg/modern-horizons/sword-of-sinew-and-steel)\n",
    "- Topic 2 - Champions of Kamigawa (https://mtg.wtf/set/chk?page=7)\n",
    "- Topic 3 - Beetleback Chief (https://gatherer.wizards.com/pages/card/Details.aspx?multiverseid=386305)\n",
    "- Topic 4 - Noxious Dragon (https://gatherer.wizards.com/pages/card/details.aspx?multiverseid=391888)\n",
    "- Topic 5 - Sarpadian Empires (https://mtg.fandom.com/wiki/Sarpadian_Empires)\n",
    "- Topic 6 - Werewolf (https://mtg.fandom.com/wiki/Werewolf)\n",
    "- Topic 7 - Vampire Lacerator (https://gatherer.wizards.com/pages/card/details.aspx?multiverseid=192225)\n",
    "- Topic 8 - Squee (Squee was a **goblin cabin-hand** on the Skyship Weatherlight - https://mtg.fandom.com/wiki/Squee)\n",
    "- Topic 9 - Necromancy (https://www.moxfield.com/decks/rlvIQMx1zUCT6smgX4GpOw)\n",
    "- Topic 10 - Garruk Wildspeaker (https://gatherer.wizards.com/pages/card/details.aspx?multiverseid=140205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdc9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add topic name\n",
    "freq_topics_11 = freq_topics.iloc[0:11, :]\n",
    "\n",
    "freq_topics_11['Topic Name'] = ['New Phyrexia',\n",
    "                                'Sword of Sinew and Steel',\n",
    "                                'Champions of Kamigawa',\n",
    "                               'Beetleback Chief',\n",
    "                               'Noxious Dragon',\n",
    "                               'Sarpadian Empires',\n",
    "                               'Werewolf',\n",
    "                               'Vampire Lacerator',\n",
    "                               'Squee',\n",
    "                               'Necromancy',\n",
    "                               'Garruk Wildspeaker']\n",
    "\n",
    "# view\n",
    "freq_topics_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(topics = [0,1,2,3,4,5,6,7,8,9,10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa84c3a",
   "metadata": {},
   "source": [
    "A heatmap shows the similarity between topics (based on the cosine similarity matrix between topic embeddings). Looking at the heatmap above, we can see that topic 9 (Necromancy) is similar to topic 4 (Noxious Dragon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23a3eb",
   "metadata": {},
   "source": [
    "#### Once you have names, create a Dynamic Topic Model by following their documentation. Use the release_date column as timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009dfefc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df.dropna(how = 'any', subset = ['flavor_text'])\n",
    "\n",
    "# check if dataframe has any missing values in the release_date column\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store release_date column as list\n",
    "timestamps = df2.release_date.to_list()\n",
    "\n",
    "# check length\n",
    "len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f677fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store flavor_text data as list\n",
    "flavor_text_list = df2.flavor_text.tolist()\n",
    "\n",
    "# check length\n",
    "len(flavor_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de64c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit model again \n",
    "topics, probs = topic_model.fit_transform(flavor_text_list)\n",
    "\n",
    "# check length of topics\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da029b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the topic representations at each timestamp for each topic \n",
    "topics_over_time = topic_model.topics_over_time(flavor_text_list, topics, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30292c5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, topics = [0,1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6f441",
   "metadata": {},
   "source": [
    "`Champions of Kamigawa` was released in October 2004 (which explains the spike around 2005). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458bc00f",
   "metadata": {},
   "source": [
    "## Part 2 Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1952f57",
   "metadata": {},
   "source": [
    "Using only the text and flavor_text data, predict the color identity of cards:\n",
    "\n",
    "Follow the sklearn documentation covered in class on text data and Pipelines to create a classifier that predicts which of the colors a card is identified as. You will need to preprocess the target color_identity labels depending on the task:\n",
    "\n",
    "- Source code for pipelines\n",
    "    - in multiclass.py, again load data and train a Pipeline that preprocesses the data and trains a multiclass classifier (LinearSVC), and saves the model pickel output once trained. target labels with more than one color should be unlabeled!\n",
    "    - in multilabel.py, do the same, but with a multilabel model (e.g. here). You should now use the original color_identity data as-is, with special attention to the multi-color cards.\n",
    "- in dvc.yaml, add these as stages to take the data and scripts as input, with the trained/saved models as output.\n",
    "\n",
    "\n",
    "- in your notebook:\n",
    "    - **Describe: preprocessing steps (the tokenization done, the ngram_range, etc.), and why.**\n",
    "    - **load both models and plot the confusion matrix for each model (see here for the multilabel-specific version)**\n",
    "    - **Describe: what are the models succeeding at? Where are they struggling? How do you propose addressing these weaknesses next time?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f050e",
   "metadata": {},
   "source": [
    "### Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec6eb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be12a8",
   "metadata": {},
   "source": [
    "`color_identity` and `text` don't have any missing values so only missing values from the `flavor_text` variable need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41aaff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where target (color_identity) or predictors (flavor_text and text) have missing values\n",
    "df2 = df.dropna(how = 'any',\n",
    "                subset = ['flavor_text'])\n",
    "\n",
    "# check\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aaa80d",
   "metadata": {},
   "source": [
    "#### For $x$, combine text and flavor text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31b36a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2['combined_text'] = df['text'] + ' ' + df['flavor_text']\n",
    "\n",
    "# view\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb9352",
   "metadata": {},
   "source": [
    "#### For $y$, encode target variable (`color_identity`)\n",
    "\n",
    "Target labels with more than one color should be unlabeled!\n",
    "\n",
    "To \"unlabel\" data, I will replace the label with -1.<br>\n",
    "Where there are no values, I will replace the label to null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store color_identity values as a list\n",
    "color_identity_values = list(df2.color_identity.values)\n",
    "\n",
    "# create empty list to store results\n",
    "color_identity_multiclass = []\n",
    "\n",
    "# iterate through list, and unlabel target labels with more than one color\n",
    "for i in color_identity_values:\n",
    "    if len(i) == 1:\n",
    "        color_identity_multiclass.append(i[0])\n",
    "    elif len(i) < 1:\n",
    "        color_identity_multiclass.append(0) # storing missing values as 0\n",
    "    else:\n",
    "        color_identity_multiclass.append(-1) # unlabeling target labels with more than one color\n",
    "\n",
    "# check length\n",
    "len(color_identity_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check target labels\n",
    "set(color_identity_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "### encode target labels (I will do this manually instead of using LabelEncoder())\n",
    "\n",
    "# store empty list to append to later\n",
    "encoded_target_multiclass = []\n",
    "\n",
    "for i in color_identity_multiclass:\n",
    "    if i == 'W':\n",
    "        encoded_target_multiclass.append(1)\n",
    "    elif i == 'U':\n",
    "        encoded_target_multiclass.append(2)\n",
    "    elif i == 'R':\n",
    "        encoded_target_multiclass.append(3)\n",
    "    elif i == 'G':\n",
    "        encoded_target_multiclass.append(4)\n",
    "    elif i == 'B':\n",
    "        encoded_target_multiclass.append(5)\n",
    "    elif i == -1:\n",
    "        encoded_target_multiclass.append(i)\n",
    "    else:\n",
    "        encoded_target_multiclass.append(i)\n",
    "        \n",
    "# check length\n",
    "len(encoded_target_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb897af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels\n",
    "set(encoded_target_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2bf3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add encoded labels to dataframe as a new column\n",
    "df2['multiclass'] = encoded_target_multiclass\n",
    "\n",
    "# view\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9bd9f",
   "metadata": {},
   "source": [
    "#### Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store target and predictor\n",
    "y = df2[['multiclass']]\n",
    "X = df2[['combined_text']]\n",
    "\n",
    "# split data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y , test_size = .25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca32e1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check training and test data shapes\n",
    "print(train_X.shape[0]/df2.shape[0])\n",
    "print(test_X.shape[0]/df2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976fc8d8",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training data as a list\n",
    "training_X = train_X.combined_text.tolist()\n",
    "\n",
    "# check length\n",
    "len(training_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train_y length\n",
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ae364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training target as numpy array\n",
    "training_target = train_y.multiclass.values\n",
    "\n",
    "# check length\n",
    "len(training_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011c2ba",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store test data as a list\n",
    "test_x = test_X.combined_text.tolist()\n",
    "\n",
    "# check length\n",
    "len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91203ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check test_y length\n",
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a088237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store test target as numpy array\n",
    "test_target = test_y.multiclass.values\n",
    "\n",
    "# check length\n",
    "len(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39269f20",
   "metadata": {},
   "source": [
    "#### Preprocessing Steps:\n",
    "\n",
    "Pre-processing text using CountVectorizer():\n",
    "- removing English stop words in order to remove the 'low-level' information in the text and focus more on the important information.\n",
    "- converting all words to lowercase - assumption is that the meaning and significance of a lowercase word is the same as when that word is in uppercase or capitalized. This will help remove noise.\n",
    "- ngram_range set to 1,2 i.e. capturing both unigrams and bigrams since Magic Card texts often have names/terms that are bigrams e.g. Soul Warden and Beetleback Chief. \n",
    "- min_df set to 5 i.e. rare words that appear in less than 5 documents will be ignored.\n",
    "- max_df set to 0.9 i.e. words that appear in more than 90% of the documents will be ignored since they are not adding much to a specific document.\n",
    "\n",
    "Using TfidfTransformer():\n",
    "- Term frequencies calculated to overcome the discrepancies with using occurence count for differently sized documents. \n",
    "- Downscaled weights for words that occur in many documents and therefore do not add a lot of information than those that occur in a smaller share of the corpus (tf-idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30654f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multiclass model\n",
    "file_to_read = open(\"multiclass_classifier.pickle\", \"rb\")\n",
    "multiclass_classifier = pickle.load(file_to_read)\n",
    "file_to_read.close()\n",
    "\n",
    "# view\n",
    "print(multiclass_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = multiclass_classifier.predict(test_x)\n",
    "np.mean(predicted == test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b214bae",
   "metadata": {},
   "source": [
    "We achieved 85% accuracy using Linear SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b2bf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "multilabel_confusion_matrix(test_target, predicted, labels = [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebad88",
   "metadata": {},
   "source": [
    "This is how we can interpret the confusion matrix values: 6023 of the observations with the label 1 (i.e. color White) were predicted correctly by the model, whereas 1007 observations that did not have the label 1 were predicted correctly by the model. 208 records that did not have the label 1 were wrongy predicted as having the label 1, while 171 records that did have the label 1 were wrongly predicted as not having the label 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2758e3",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('metrics.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    " \n",
    "# print\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16729ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store scores as a dataframe\n",
    "metrics = pd.DataFrame(metrics.classification_report(test_target, predicted, output_dict = True))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f3ad9",
   "metadata": {},
   "source": [
    "The macro-averaged F1-score is computed as a simple arithmetic mean of the per-class F1-scores.\n",
    "\n",
    "When averaging the macro-F1, we gave equal weights to each class. We don’t have to do that: in weighted-average F1-score, we weight the F1-score of each class by the number of samples from that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f816bd",
   "metadata": {},
   "source": [
    "### Multilabel Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7198334",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ff912",
   "metadata": {},
   "source": [
    "`color_identity` and `text` don't have any missing values so only missing values from the `flavor_text` variable need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where target (color_identity) or predictors (flavor_text and text) have missing values\n",
    "df2 = df.dropna(how = 'any',\n",
    "                subset = ['flavor_text'])\n",
    "\n",
    "# check\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b19051",
   "metadata": {},
   "source": [
    "#### For $x$, combine text and flavor text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f7249",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2['combined_text'] = df['text'] + ' ' + df['flavor_text']\n",
    "\n",
    "# view\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa33ed",
   "metadata": {},
   "source": [
    "#### For $y$, use the (`color_identity`) column as is\n",
    "\n",
    "Guidance obtained from: https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fa925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store color_identity values as a list\n",
    "color_identity_values = list(df2.color_identity.values)\n",
    "\n",
    "# create label binary indicator array - target\n",
    "color_identity_multilabels = MultiLabelBinarizer().fit_transform(color_identity_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store target and predictor\n",
    "y = color_identity_multilabels\n",
    "X = df2[['combined_text']]\n",
    "\n",
    "# split data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y , test_size = .25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee687b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check training and test data shapes\n",
    "print(train_X.shape[0]/df2.shape[0])\n",
    "print(test_X.shape[0]/df2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90c5b7",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6592946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training data as a list\n",
    "training_X = train_X.combined_text.tolist()\n",
    "\n",
    "# check length\n",
    "len(training_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train_y length\n",
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbf99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training target as numpy array\n",
    "training_target = train_y\n",
    "\n",
    "# check length\n",
    "len(training_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea1775",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce096838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store test data as a list\n",
    "test_x = test_X.combined_text.tolist()\n",
    "\n",
    "# check length\n",
    "len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b1fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check test_y length\n",
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ca88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store test target as numpy array\n",
    "test_target = test_y\n",
    "\n",
    "# check length\n",
    "len(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d7512",
   "metadata": {},
   "source": [
    "#### Preprocessing Steps:\n",
    "\n",
    "Pre-processing text using CountVectorizer():\n",
    "- removing English stop words in order to remove the 'low-level' information in the text and focus more on the important information.\n",
    "- converting all words to lowercase - assumption is that the meaning and significance of a lowercase word is the same as when that word is in uppercase or capitalized. This will help remove noise.\n",
    "- ngram_range set to 1,2 i.e. capturing both unigrams and bigrams since Magic Card texts often have names/terms that are bigrams e.g. Soul Warden and Beetleback Chief. \n",
    "- min_df set to 5 i.e. rare words that appear in less than 5 documents will be ignored.\n",
    "- max_df set to 0.9 i.e. words that appear in more than 90% of the documents will be ignored since they are not adding much to a specific document.\n",
    "\n",
    "Using TfidfTransformer():\n",
    "- Term frequencies calculated to overcome the discrepancies with using occurence count for differently sized documents. \n",
    "- Downscaled weights for words that occur in many documents and therefore do not add a lot of information than those that occur in a smaller share of the corpus (tf-idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21860e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilabel model\n",
    "file_to_read = open(\"multilabel_classifier.pickle\", \"rb\")\n",
    "multilabel_classifier = pickle.load(file_to_read)\n",
    "file_to_read.close()\n",
    "\n",
    "# view\n",
    "print(multilabel_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed70db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = multilabel_classifier.predict(test_x)\n",
    "np.mean(predicted == test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0662c8",
   "metadata": {},
   "source": [
    "We achieved 93% accuracy using OneVsRestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617f109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "multilabel_confusion_matrix(test_target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032189f",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9202150",
   "metadata": {},
   "source": [
    "#### Part 3: Regression?\n",
    "\n",
    "Can we predict the EDHREC \"rank\" of the card using the data we have available?\n",
    "\n",
    "- Like above, add a script and dvc stage to create and train your model\n",
    "- in the notebook, aside from your descriptions, plot the predicted vs. actual rank, with a 45-deg line showing what \"perfect prediction\" should look like.\n",
    "- This is a freeform part, so think about the big picture and keep track of your decisions:\n",
    "    - what model did you choose? Why?\n",
    "    - What data did you use from the original dataset? How did you proprocess it?\n",
    "    - Can we see the importance of those features? e.g. logistic weights?\n",
    "- How did you do? What would you like to try if you had more time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e1951",
   "metadata": {},
   "source": [
    "For this part, I wanted to try using some categorical variables that I thought could be important predictors - namely the block i.e. sets with \"shared mechanics\", and the rarity of cards. \n",
    "\n",
    "I ran a grid search using K-nearest neighbors, random forest and a decision tree regressor, and found KNN() with 5-nearest neighbors to be the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c108c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove rows where target or predictors have missing values\n",
    "df2 = df.dropna(how = 'any',\n",
    "                subset = ['block',\n",
    "                         'rarity',\n",
    "                         'edhrec_rank'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee19bd",
   "metadata": {},
   "source": [
    "`block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a904e9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get dummies\n",
    "block_dummies = pd.get_dummies(df2.block)\n",
    "block_dummies.columns = [c.lower().replace(\" \",\"_\") for c in block_dummies.columns]\n",
    "\n",
    "block_dummies = block_dummies.drop(['alara'],axis=1) # Baseline\n",
    "block_dummies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df2.drop(['block'],axis=1),block_dummies],axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281efe6",
   "metadata": {},
   "source": [
    "`rarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc7f01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get dummies\n",
    "rarity_dummies = pd.get_dummies(df2.rarity)\n",
    "rarity_dummies.columns = [c.lower().replace(\" \",\"_\") for c in rarity_dummies.columns]\n",
    "\n",
    "rarity_dummies = rarity_dummies.drop(['common'],axis=1) # Baseline\n",
    "rarity_dummies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69029914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df2.drop(['rarity'],axis=1),rarity_dummies],axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9605e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store target and predictor\n",
    "y = df2[['edhrec_rank']]\n",
    "X = df2[['amonkhet', 'arena_league',\n",
    "       'battle_for_zendikar', 'commander', 'conspiracy', 'core_set',\n",
    "       'friday_night_magic', 'guilds_of_ravnica', 'ice_age', 'innistrad',\n",
    "       'innistrad:_double_feature', 'invasion', 'ixalan', 'judge_gift_cards',\n",
    "       'kaladesh', 'kamigawa', 'khans_of_tarkir', 'lorwyn',\n",
    "       'magic_player_rewards', 'masques', 'mirage', 'mirrodin', 'odyssey',\n",
    "       'onslaught', 'ravnica', 'return_to_ravnica', 'scars_of_mirrodin',\n",
    "       'shadowmoor', 'shadows_over_innistrad', 'tempest', 'theros',\n",
    "       'time_spiral', 'urza', 'zendikar', 'rare', 'uncommon']]\n",
    "\n",
    "# split data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y , test_size = .25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dad828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "file_to_read = open(\"best_mod.pickle\", \"rb\")\n",
    "best_mod = pickle.load(file_to_read)\n",
    "file_to_read.close()\n",
    "\n",
    "# view\n",
    "print(best_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc62a4b",
   "metadata": {},
   "source": [
    "And Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4e17e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_mod.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7cc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_mod.predict(test_X)\n",
    "np.mean(predicted == test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca61861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store test_y\n",
    "df_plot = test_y.copy()\n",
    "\n",
    "# create empty list\n",
    "predictions = []\n",
    "\n",
    "# iterate\n",
    "for i in predicted:\n",
    "    predictions.append(i[0])\n",
    "    \n",
    "# store list as dataframe column\n",
    "df_plot['predicted'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12114318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "(ggplot(data = df_plot,\n",
    "        mapping = aes(x = 'edhrec_rank', y = 'predicted')) +\n",
    " geom_point(color = 'slategray', alpha = 0.7) + \n",
    " geom_abline(intercept = 0, slope = 1, size = 2, color = 'maroon') +\n",
    " theme_minimal() +\n",
    " labs(title = 'Predicted vs Actual Rank\\n',\n",
    "     y = 'Predicted\\n',\n",
    "     x = '\\nActual')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3caacc",
   "metadata": {},
   "source": [
    "This isn't a good plot since the dots are scattered everywhere instead of being close to the line (i.e. predictions being close to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc61a9",
   "metadata": {},
   "source": [
    "KNN() with 5 nearest neighbors was identified as the best model when I did a grid search. However, when I loaded the model in the notebook, it did not have the number of neighbors specified and I was unsure how to add it or how to save the model in the .py script such that the number of neighbors also gets saved as a parameter of KNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37d184",
   "metadata": {},
   "source": [
    "How did I do? Not too great. Definitely a lot of room for improvement. I would like to select more predictors if I have more time, as well as include k=5 in the KNN regressor (update: the default number of neighbors is 5 so even though I didn't specify k, the model ran with k=5). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da73e1e6",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cac55c",
   "metadata": {},
   "source": [
    "### For multiclass, report average and F1\n",
    "Done above where the multiclass model was run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1275eef9",
   "metadata": {},
   "source": [
    "### Run a new experiment that changes one parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146c224",
   "metadata": {},
   "source": [
    "#### output of `dvc exp diff` copy and pasted from the command line, and formatted to a table:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2972fe",
   "metadata": {},
   "source": [
    "|Path    |      Metric        |          exp-ddb8e   | workspace  |  Change |\n",
    "|   -    |         -          |         -            |     -      |    -    |\n",
    "|metrics.json | -1.f1-score            | 0.78642     | 0.77876    |  -0.0076563 \n",
    "|metrics.json | -1.precision           | 0.81949     | 0.81064    |  -0.0088423 \n",
    "|metrics.json | -1.recall              | 0.75591     | 0.74929    |  -0.0066225 \n",
    "|metrics.json | 0.f1-score             | 0.8684      | 0.86119    |  -0.0072075  \n",
    "|metrics.json | 0.precision            | 0.90123     | 0.89736    |  -0.0038784 \n",
    "|metrics.json | 0.recall               | 0.83788     | 0.82783    |  -0.010043 \n",
    "|metrics.json | 1.f1-score             | 0.83958     | 0.84162    |  0.0020424\n",
    "|metrics.json | 1.precision            | 0.83292     | 0.82881    |  -0.004109\n",
    "|metrics.json | 1.recall               | 0.84635     | 0.85484    |  0.008489\n",
    "|metrics.json | 2.f1-score             | 0.87825     | 0.87506    |  -0.0031947\n",
    "|metrics.json | 2.precision            | 0.85212     | 0.84865    |  -0.0034704\n",
    "|metrics.json | 2.recall               | 0.90604     | 0.90316    |  -0.0028763\n",
    "|metrics.json | 3.f1-score             | 0.8846      | 0.88317    |  -0.0014276\n",
    "|metrics.json | 3.precision            | 0.86964     | 0.8725     |  0.002863\n",
    "|metrics.json | 3.recall               | 0.90009     | 0.89411    |  -0.0059778\n",
    "|metrics.json | 4.f1-score             | 0.85777     | 0.85627    |  -0.0014944\n",
    "|metrics.json | 4.precision            | 0.85702     | 0.85404    |  -0.0029783\n",
    "|metrics.json | 5.f1-score             | 0.85931     | 0.85253    |  -0.0067797\n",
    "|metrics.json | 5.precision            | 0.85816     | 0.85445    |  -0.0037149\n",
    "|metrics.json | 5.recall               | 0.86047     | 0.85063    |  -0.009839\n",
    "|metrics.json | accuracy               | 0.85356     | 0.85018    |  -0.0033743\n",
    "|metrics.json | macro avg.f1-score     | 0.85348     | 0.8498     |  -0.0036739\n",
    "|metrics.json | macro avg.precision    | 0.8558      | 0.85235    |  -0.0034472\n",
    "|metrics.json | macro avg.recall       | 0.85218     | 0.84834    |  -0.0038385\n",
    "|metrics.json | weighted avg.f1-score  | 0.85305     | 0.84968    |  -0.0033749\n",
    "|metrics.json | weighted avg.precision | 0.85347     | 0.85013    |  -0.0033366\n",
    "|metrics.json | weighted avg.recall    | 0.85356     | 0.85018    |  -0.0033743\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "|Path     |    Param             |            exp-ddb8e  |  workspace  |  Change |\n",
    "|-|-|-|-|-|\n",
    "| params.yaml | preprocessing.ngrams.largest | 3 |           2  |          -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b14c0",
   "metadata": {},
   "source": [
    "Grabbing the weighted average scores from the output above:\n",
    "\n",
    "| | Precision | Recall | F1-Score | \n",
    "| --- | --- | --- | --- |\n",
    "| ngrams.largest = 2|0.85013|0.85018|0.84968|\n",
    "| ngrams.largest = 3| 0.85347 | 0.85356 | 0.85305 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f07379",
   "metadata": {},
   "source": [
    "There was only a very slight improvement in performance when the ngram range was changed from (1,2) to (1,3), based on the slightly higher scores. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test_env]",
   "language": "python",
   "name": "conda-env-test_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
