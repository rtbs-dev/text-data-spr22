{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ce53e6",
   "metadata": {},
   "source": [
    "# Homework: Goals & Approaches\n",
    "\n",
    "> The body grows stronger under stress. The mind does not.\n",
    "> \n",
    ">  -- Magic the Gathering, _Fractured Sanity_\n",
    "\n",
    "This homework deals with the goals you must define, along with the approaches you deem necessary to achieve those goals. \n",
    "Key to this will be a focus on your _workflows_: \n",
    "\n",
    "- are they reproducible? \n",
    "- are they maintainable? \n",
    "- are they well-justified and communicated? \n",
    "\n",
    "This is not a \"machine-learning\" course, but machine learning plays a large part in modern text analysis and NLP. \n",
    "Machine learning, in-turn, has a number of issues tracking and solving issues in a collaborative, asynchronous, distributed manner. \n",
    "\n",
    "It's not inherently _wrong_ to use pre-configured models and libraries! \n",
    "In fact, you will likely be unable to create a set of ML algorithms that \"beat\" something others have spent 100's of hours creating, optimizing, and validating. \n",
    "However, to answer the three questions above, we need a way to explicitly track our decisions to use others' work, and efficiently _swap out_ that work for new ideas and directions as the need arises. \n",
    "\n",
    "This homework is a \"part 1\" of sorts, where you will construct several inter-related pipelines in a way that will allow _much easier_ adjustment, experimentation, and measurement in \"part 2\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641ba71",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Dependencies \n",
    "As before, ensure you have an up-to-date environment to isolate your work. \n",
    "Use the `environment.yml` file in the project root to create/update the `text-data-class` environment. \n",
    "> I expect any additional dependencies to be added here, which will show up on your pull-request. \n",
    "\n",
    "### Data\n",
    "Once again, we have set things up to use DVC to import our data. \n",
    "If the data changes, things will automatically update! \n",
    "The data for this homework has been imported as `mtg.feather` under the `data/` directory at the top-level of this repository. \n",
    "In order to ensure your local copy of the repo has the actual data (instead of just the `mtg.feather.dvc` stub-file), you need to run `dvc pull`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a606a",
   "metadata": {},
   "source": [
    "Then you may load the data into your notebooks and scripts e.g. using pandas+pyarrow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "(pd.read_feather('../../data/mtg.feather')# <-- will need to change for your notebook location\n",
    " .head()[['name','text', 'mana_cost', 'flavor_text','release_date', 'edhrec_rank']]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e06cf",
   "metadata": {},
   "source": [
    "But that's not all --- at the end of this homework, we will be able to run a `dvc repro` command and all of our main models and results will be made available for your _notebook_ to open and display. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5d788",
   "metadata": {},
   "source": [
    "### Submission Structure\n",
    "You will need to submit a pull-request on DagsHub with the following additions: \n",
    "\n",
    "- your subfolder, e.g. named with your user id, inside the `homework/hw2-goals-approaches/` folder\n",
    "    - your \"lab notebook\", as an **`.ipynb` or `.md`** (e.g. jupytext), that will be exported to PDF for Canvas submission. **This communicates your _goals_**, along with the results that will be compared to them. \n",
    "    - your **`dvc.yaml`** file that will define  the inputs and outputs of your _approaches_. See [the DVC documentation](https://dvc.org/doc/user-guide/project-structure/pipelines-files) for information!\n",
    "    - **source code** and **scripts** that define the preprocessing and prediction `Pipeline`'s you wish to create. You may then _print_ the content of those scripts at the end of your notebook e.g. as appendices using \n",
    "- any updates to `environment.yml` to add the dependencies you want to use for this homework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bca163",
   "metadata": {},
   "source": [
    "## Part 1: Unsupervised Exploration\n",
    "\n",
    "Investigate the [BERTopic](https://maartengr.github.io/BERTopic/index.html) documentation (linked), and train a model using their library to create a topic model of the `flavor_text` data in the dataset above. \n",
    "\n",
    "- In a `topic_model.py`, load the data and train a bertopic model. You will `save` the model in that script as a new trained model object\n",
    "- add a \"topic-model\" stage to your `dvc.yaml` that has `mtg.feather` and `topic_model.py` as dependencies, and your trained model as an output\n",
    "- load the trained bertopic model into your notebook and display\n",
    "    1. the `topic_visualization` interactive plot [see docs](https://maartengr.github.io/BERTopic/api/plotting/topics.html)\n",
    "    2. Use the plot to come up with working \"names\" for each major topic, adjusting the _number_ of topics as necessary to make things more useful. \n",
    "    3. Once you have names, create a _Dynamic Topic Model_ by following [their documentation](https://maartengr.github.io/BERTopic/getting_started/topicsovertime/topicsovertime.html). Use the `release_date` column as timestamps. \n",
    "    4. Describe what you see, and any possible issues with the topic models BERTopic has created. **This is the hardest part... interpreting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6aa7eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:  67%|██████▋   | 625/927 [07:16<02:43,  1.85it/s]"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# load model and display \n",
    "topic_model = BERTopic.load(\"my_model\")\n",
    "\n",
    "# load topic model variables\n",
    "from topic_model import mtg as text\n",
    "from topic_model import topics as topics\n",
    "from topic_model import probs as probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mtg = pd.read_feather('../../../data/mtg.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76be18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12369c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6839660",
   "metadata": {},
   "source": [
    "# what are some working names for each of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_date = mtg[['release_date', 'flavor_text']].dropna().drop(columns=['flavor_text'])\n",
    "\n",
    "# convert release date to list\n",
    "release_date = list(release_date['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21744be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dynamic topic model using release date columns as timestamps\n",
    "topics_over_time = topic_model.topics_over_time(text, topics, release_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe! All the topics increase over time that's not helpful\n",
    "# normalize frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be417e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_over_time.groupby('Timestamp')['Frequency'].transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e54c03",
   "metadata": {},
   "source": [
    "## Part 2 Supervised Classification\n",
    "\n",
    "Using only the `text` and `flavor_text` data, predict the color identity of cards: \n",
    "\n",
    "Follow the sklearn documentation covered in class on text data and Pipelines to create a classifier that predicts which of the colors a card is identified as. \n",
    "You will need to preprocess the target _`color_identity`_ labels depending on the task: \n",
    "\n",
    "- Source code for pipelines\n",
    "    - in `multiclass.py`, again load data and train a Pipeline that preprocesses the data and trains a multiclass classifier (`LinearSVC`), and saves the model pickel output once trained. target labels with more than one color should be _unlabeled_! \n",
    "    - in `multilabel.py`, do the same, but with a multilabel model (e.g. [here](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_multilabel.html#sphx-glr-auto-examples-miscellaneous-plot-multilabel-py)). You should now use the original `color_identity` data as-is, with special attention to the multi-color cards. \n",
    "- in `dvc.yaml`, add these as stages to take the data and scripts as input, with the trained/saved models as output. \n",
    "\n",
    "- in your notebook: \n",
    "    - Describe:  preprocessing steps (the tokenization done, the ngram_range, etc.), and why. \n",
    "    - load both models and plot the _confusion matrix_ for each model ([see here for the multilabel-specific version](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html))\n",
    "    - Describe: what are the models succeeding at? Where are they struggling? How do you propose addressing these weaknesses next time?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ed6e3",
   "metadata": {},
   "source": [
    "Describe the pre-processing steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874439a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrixes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3584f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiclass import X_test as X_mc, y_test as y_mc\n",
    "from multilabel import X_test as X_ml, y_test as y_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6308bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"mtg_classifier_multilabel.pkl\",'rb')\n",
    "mc_classifier = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"mtg_classifier.pkl\",'rb')\n",
    "ml_classifier = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ccfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_mc_pred = mc_classifier.predict(X_mc)\n",
    "y_ml_pred = ml_classifier.predict(X_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrixes\n",
    "print(\"Multiclass confusion matrix:\")\n",
    "print(confusion_matrix(y_mc, y_mc_pred))\n",
    "\n",
    "print(\"Multilabel confusion matrix:\")\n",
    "print(multilabel_confusion_matrix(y_ml, y_ml_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot multiclass confusion matrix\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_mc, y_mc_pred)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ff3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilabel confusion matrix plot\n",
    "ConfusionMatrixDisplay(confusion_matrix=multilabel_confusion_matrix(y_ml, y_ml_pred)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How are they doing? Let's look at scores!\n",
    "\n",
    "# multilabel\n",
    "from multilabel import f1, precision, recall\n",
    "print(f1)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04b35e",
   "metadata": {},
   "source": [
    "Really good at precision, eh at recall, not bad overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc123a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass\n",
    "from multiclass import f1, precision, recall\n",
    "print(f1)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab0fb3",
   "metadata": {},
   "source": [
    "pretty good all around- balanced between precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474852ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02985dfa",
   "metadata": {},
   "source": [
    "## Part 3: Regression?\n",
    "\n",
    "> Can we predict the EDHREC \"rank\" of the card using the data we have available? \n",
    "\n",
    "- Like above, add a script and dvc stage to create and train your model\n",
    "- in the notebook, aside from your descriptions, plot the `predicted` vs. `actual` rank, with a 45-deg line showing what \"perfect prediction\" should look like. \n",
    "- This is a freeform part, so think about the big picture and keep track of your decisions: \n",
    "    - what model did you choose? Why? \n",
    "    - What data did you use from the original dataset? How did you proprocess it? \n",
    "    - Can we see the importance of those features? e.g. logistic weights? \n",
    "    \n",
    "How did you do? What would you like to try if you had more time? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted vs actual\n",
    "import matplotlib.pyplot as plt\n",
    "file = open(\"regression.pkl\",'rb')\n",
    "clf = pickle.load(file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eaea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regression import y_test, X_test \n",
    "\n",
    "#predict the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Measured\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7db49c",
   "metadata": {},
   "source": [
    "This is garbage- maybe try lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434f99e",
   "metadata": {},
   "source": [
    "#### what regression model did I choose?\n",
    "linear regression in part because it's easily interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659aafb2",
   "metadata": {},
   "source": [
    "#### what data do we use and how did I process it? \n",
    "\n",
    "first selection based just off what sounds important: \n",
    "mtg = pd.read_feather('../../../data/mtg.feather')[['edhrec_rank', 'color_identity', 'converted_mana_cost', 'power', 'toughness', 'rarity', 'subtypes', \\\n",
    "'supertypes', 'types', 'text', 'flavor_text', 'life', 'block']]\n",
    "\n",
    "Lots of NAs in power, toughness, and life so I got rid of those\n",
    "I thought we could make categoires for supertypes and subtypes but there were so many that it didn't really amke sense to do so so I then got rid of those\n",
    "Turned rarity into a bynch of dummies as well as blocks and types (33 possible) \n",
    "32 possible color combos so made them dummies too\n",
    "- this is relying on a lot of dummies\n",
    "\n",
    "Combined flavor text and text into one column, vectorized, made a tfidf dtm and appended that to the dataframe \n",
    "\n",
    "train test split of 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c03734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance!\n",
    "# get importance in descending order\n",
    "import numpy as np\n",
    "importance = np.sort(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6039649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f64f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.concat([pd.DataFrame(X_test.columns), pd.DataFrame(importance)], axis=1)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize feature importance\n",
    "for i,v in enumerate(importance[1:10]):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.xlabel('Features')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "470f6f016bf309f80cf8017155ce6d7ccfc6ca9fae0b4a451c57f17a587998f9"
  },
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:text-data-class]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
